#!/usr/bin/env python3
"""
Final Fact-Checking Assessment - Post-Implementation Validation
Using Fact-Checking Skill for Quality Assurance
"""

import os
import json
from datetime import datetime

def final_fact_check_assessment():
    """Final validation of Mini-Agent system corrections."""
    
    assessment = {
        "timestamp": datetime.now().isoformat(),
        "assessment_type": "Final Post-Implementation Validation",
        "confidence_score": 0,
        "issues_resolved": [],
        "issues_remaining": [],
        "verification_results": {},
        "recommendations": []
    }
    
    print("ğŸ” Running Final Fact-Checking Assessment...")
    print("="*60)
    
    # 1. Z.AI Tools Implementation Verification
    try:
        print("\nğŸ“¡ Checking Z.AI Tools Implementation...")
        
        with open("mini_agent/tools/zai_tools.py", "r") as f:
            tools_content = f.read()
        
        # Check for resolved issues
        zai_fixes_verified = []
        
        # Token usage fix
        if "Direct search API does not report token usage" in tools_content:
            zai_fixes_verified.append("âœ… Token usage issue resolved")
        elif "token_usage" in tools_content and "N/A" in tools_content:
            zai_fixes_verified.append("âŒ Token usage still incorrectly displayed")
        else:
            zai_fixes_verified.append("âœ… Token usage properly handled")
        
        # Model reference fix
        if "glm-4-air" not in tools_content:
            zai_fixes_verified.append("âœ… glm-4-air reference removed")
        else:
            zai_fixes_verified.append("âŒ glm-4-air reference still present")
        
        # Accept-Language header
        with open("mini_agent/llm/zai_client.py", "r") as f:
            client_content = f.read()
        if "Accept-Language" in client_content:
            zai_fixes_verified.append("âœ… Accept-Language header present")
        else:
            zai_fixes_verified.append("âŒ Accept-Language header missing")
        
        # Reader endpoint
        if '"/reader"' in client_content:
            zai_fixes_verified.append("âœ… Correct /reader endpoint used")
        else:
            zai_fixes_verified.append("âŒ Incorrect reader endpoint")
        
        assessment["verification_results"]["zai_implementation"] = zai_fixes_verified
        print(f"   Z.AI Implementation: {len([f for f in zai_fixes_verified if f.startswith('âœ…')])}/{len(zai_fixes_verified)} fixes verified")
        
    except Exception as e:
        assessment["verification_results"]["zai_implementation"] = [f"âŒ Error checking Z.AI: {str(e)}"]
    
    # 2. Document Hygiene Verification
    try:
        print("\nğŸ“š Checking Document Hygiene...")
        
        required_docs = [
            ("documents/PROJECT_CONTEXT.md", "PROJECT_CONTEXT.md"),
            ("documents/SETUP_GUIDE.md", "SETUP_GUIDE.md"),
            ("documents/AGENT_HANDOFF.md", "AGENT_HANDOFF.md")
        ]
        
        doc_checks = []
        for file_path, doc_name in required_docs:
            if os.path.exists(file_path):
                doc_checks.append(f"âœ… {doc_name} exists")
            else:
                doc_checks.append(f"âŒ {doc_name} missing")
        
        # Check for misplaced files
        root_files = os.listdir(".")
        misplaced_files = [f for f in root_files if f.endswith('.py') and (f.startswith('test_') or f.startswith('demo') or f.startswith('check'))]
        if misplaced_files:
            doc_checks.append(f"âŒ Misplaced test files in root: {misplaced_files}")
        else:
            doc_checks.append("âœ… No misplaced files in root directory")
        
        assessment["verification_results"]["document_hygiene"] = doc_checks
        print(f"   Document Structure: {len([f for f in doc_checks if f.startswith('âœ…')])}/{len(doc_checks)} requirements met")
        
    except Exception as e:
        assessment["verification_results"]["document_hygiene"] = [f"âŒ Error checking docs: {str(e)}"]
    
    # 3. System Architecture Verification
    try:
        print("\nğŸ—ï¸  Checking System Architecture Alignment...")
        
        # Check if agent properly identifies as CLI/coder
        try:
            with open("mini_agent/agent.py", "r", encoding='utf-8') as f:
                agent_content = f.read()
            arch_checks = ["âœ… agent.py readable (UTF-8)"]
        except:
            arch_checks = ["âŒ agent.py encoding issues"]
        
        # Check core files exist
        core_files = ["mini_agent/cli.py", "mini_agent/llm/zai_client.py", "mini_agent/tools/zai_tools.py"]
        for file_path in core_files:
            if os.path.exists(file_path):
                arch_checks.append(f"âœ… {file_path} exists")
            else:
                arch_checks.append(f"âŒ {file_path} missing")
        
        assessment["verification_results"]["architecture_alignment"] = arch_checks
        print(f"   Architecture: {len([f for f in arch_checks if f.startswith('âœ…')])}/{len(arch_checks)} components verified")
        
    except Exception as e:
        assessment["verification_results"]["architecture_alignment"] = [f"âŒ Error checking architecture: {str(e)}"]
    
    # 4. Calculate Overall Confidence Score
    all_results = []
    for category, results in assessment["verification_results"].items():
        all_results.extend(results)
    
    passed_checks = len([r for r in all_results if r.startswith('âœ…')])
    total_checks = len(all_results)
    
    if total_checks > 0:
        assessment["confidence_score"] = int((passed_checks / total_checks) * 100)
    
    # 5. Generate Summary
    print("\n" + "="*60)
    print("ğŸ“Š FINAL ASSESSMENT RESULTS")
    print("="*60)
    print(f"â° Assessment Time: {assessment['timestamp']}")
    print(f"ğŸ¯ Confidence Score: {assessment['confidence_score']}/100")
    print(f"âœ… Checks Passed: {passed_checks}/{total_checks}")
    
    if assessment["confidence_score"] >= 90:
        print("\nğŸŒŸ SYSTEM READY FOR PRODUCTION")
    elif assessment["confidence_score"] >= 70:
        print("\nâœ… SYSTEM MOSTLY READY - Minor improvements needed")
    else:
        print("\nâš ï¸  SYSTEM NEEDS ATTENTION - Multiple issues found")
    
    # 6. Save Assessment
    with open("documents/testing/final_assessment_results.json", "w") as f:
        json.dump(assessment, f, indent=2)
    
    print(f"\nğŸ“„ Detailed results saved to: documents/testing/final_assessment_results.json")
    print("="*60)
    
    return assessment

if __name__ == "__main__":
    final_assessment = final_fact_check_assessment()
    
    # Print detailed results
    print("\nğŸ” DETAILED VERIFICATION RESULTS:")
    for category, results in final_assessment["verification_results"].items():
        print(f"\n{category.upper().replace('_', ' ')}:")
        for result in results:
            print(f"   {result}")