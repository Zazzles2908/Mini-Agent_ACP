#!/usr/bin/env python3
"""
End-to-End Observability System Test
Tests the complete Mini-Agent observability pipeline including:
- Langfuse integration
- Supabase integration  
- Tool instrumentation
- Performance metrics
- Agent decision tracking
"""

import os
import time
import json
from datetime import datetime
from pathlib import Path

# Import our observability components
from client import MiniAgentObservability
from instrumentation import instrument_tool, instrument_class, log_performance, log_decision

def test_file_operations():
    """Test file-related tool operations"""
    
    @instrument_tool(category="file")
    def create_file(path: str, content: str):
        """Create a test file"""
        time.sleep(0.05)  # Simulate I/O
        return {"created": True, "path": path, "size": len(content)}
    
    @instrument_tool(category="file")
    def read_file(path: str):
        """Read a test file"""
        time.sleep(0.03)  # Simulate I/O
        return {"content": f"Sample content for {path}", "lines": 3}
    
    @instrument_tool(category="file")
    def delete_file(path: str):
        """Delete a test file"""
        time.sleep(0.02)  # Simulate I/O
        return {"deleted": True, "path": path}
    
    print("Testing file operations...")
    
    # Test file lifecycle
    create_result = create_file("/test/observability.txt", "Test content for observability system")
    read_result = read_file("/test/observability.txt")
    delete_result = delete_file("/test/observability.txt")
    
    return {
        "create": create_result,
        "read": read_result,
        "delete": delete_result
    }

def test_mcp_operations():
    """Test MCP-related tool operations"""
    
    @instrument_tool(category="mcp")
    def query_supabase(table: str, filters: dict = None):
        """Simulate Supabase query"""
        time.sleep(0.1)  # Simulate network delay
        return {
            "table": table,
            "filters": filters,
            "results": [{"id": 1, "name": "test"}],
            "count": 1
        }
    
    @instrument_tool(category="mcp")
    def call_external_api(endpoint: str, method: str = "GET"):
        """Simulate external API call"""
        time.sleep(0.15)  # Simulate network delay
        return {
            "endpoint": endpoint,
            "method": method,
            "status": 200,
            "response": {"data": "API response"}
        }
    
    print("Testing MCP operations...")
    
    # Test MCP operations
    supabase_result = query_supabase("agent_traces", {"success": True})
    api_result = call_external_api("/api/status", "GET")
    
    return {
        "supabase": supabase_result,
        "api": api_result
    }

def test_agent_decisions():
    """Test agent decision logging"""
    
    # Tool selection decision
    log_decision(
        decision_type="tool_selection",
        considered=["read_file", "bash.cat", "smart_file_query"],
        selected="read_file",
        reasoning="Need to read local file content directly",
        confidence=0.95
    )
    
    # Parameter choice decision
    log_decision(
        decision_type="parameter_choice",
        considered=["exact", "fuzzy", "partial"],
        selected="exact",
        reasoning="Exact matching required for precise file operations",
        confidence=0.88
    )
    
    # Error handling decision
    log_decision(
        decision_type="error_handling",
        considered=["retry", "fallback", "skip"],
        selected="retry",
        reasoning="Network errors are typically transient",
        confidence=0.75
    )
    
    print("Agent decisions logged")

def test_performance_metrics():
    """Test performance metric collection"""
    
    # System performance metrics
    log_performance("memory_usage", 512.3, "MB")
    log_performance("cpu_usage", 23.7, "%")
    log_performance("disk_usage", 45.2, "%")
    
    # Application-specific metrics
    log_performance("response_time", 156.7, "ms")
    log_performance("throughput", 42.1, "req/sec")
    log_performance("error_rate", 0.02, "%")
    
    # AI-specific metrics
    log_performance("token_usage", 1250, "tokens")
    log_performance("latency", 2100, "ms")
    log_performance("cost", 0.045, "USD")
    
    print("Performance metrics logged")

def test_class_instrumentation():
    """Test class method instrumentation"""
    
    class TestAgent:
        def __init__(self):
            self.operations = []
        
        def plan_action(self, task: str):
            """Plan an action (should be instrumented)"""
            time.sleep(0.05)
            self.operations.append(f"planned_{task}")
            return {"planned": True, "task": task}
        
        def execute_action(self, action: dict):
            """Execute an action (should be instrumented)"""
            time.sleep(0.1)
            result = f"executed_{action['task']}"
            self.operations.append(result)
            return {"executed": True, "result": result}
        
        def report_status(self):
            """Report status (should be instrumented)"""
            return {"status": "ready", "operations": len(self.operations)}
    
    print("Testing class instrumentation...")
    
    # Create and instrument agent
    agent = TestAgent()
    instrument_class(agent, category="agent", method_pattern="*action*")
    
    # Use the agent
    plan = agent.plan_action("file_processing")
    execute = agent.execute_action(plan)
    status = agent.report_status()
    
    return {
        "plan": plan,
        "execute": execute,
        "status": status
    }

def test_error_scenarios():
    """Test error handling and logging"""
    
    @instrument_tool(category="error_test")
    def failing_operation():
        """An operation that deliberately fails"""
        raise ValueError("Test error for observability")
    
    @instrument_tool(category="error_test") 
    def slow_operation():
        """An operation with timeout simulation"""
        time.sleep(0.2)  # Simulate slow operation
        return {"status": "completed", "duration": "200ms"}
    
    print("Testing error scenarios...")
    
    # Test successful slow operation
    slow_result = slow_operation()
    
    # Test failing operation (this will raise an exception)
    try:
        failing_operation()
    except ValueError as e:
        print(f"Expected error caught: {e}")
    
    return {
        "slow_operation": slow_result,
        "error_handled": True
    }

def main():
    """Run comprehensive observability system test"""
    
    print("üéØ Mini-Agent Observability System - End-to-End Test")
    print("=" * 60)
    print(f"Test Time: {datetime.now()}")
    print()
    
    # Initialize observability client
    print("Initializing observability client...")
    client = MiniAgentObservability()
    
    # Get initial session info
    session_info = client.get_session_summary()
    print(f"Session ID: {session_info['session_id']}")
    print(f"Langfuse available: {session_info['langfuse_available']}")
    print(f"Supabase available: {session_info['supabase_available']}")
    print()
    
    test_results = {}
    
    # Test 1: File operations
    print("üîß Test 1: File Operations")
    test_results['file_operations'] = test_file_operations()
    print()
    
    # Test 2: MCP operations
    print("üîß Test 2: MCP Operations")  
    test_results['mcp_operations'] = test_mcp_operations()
    print()
    
    # Test 3: Agent decisions
    print("üîß Test 3: Agent Decision Tracking")
    test_results['agent_decisions'] = test_agent_decisions()
    print()
    
    # Test 4: Performance metrics
    print("üîß Test 4: Performance Metrics")
    test_results['performance_metrics'] = test_performance_metrics()
    print()
    
    # Test 5: Class instrumentation
    print("üîß Test 5: Class Method Instrumentation")
    test_results['class_instrumentation'] = test_class_instrumentation()
    print()
    
    # Test 6: Error scenarios
    print("üîß Test 6: Error Handling")
    test_results['error_scenarios'] = test_error_scenarios()
    print()
    
    # Generate performance summary
    print("üìä Performance Summary")
    total_duration = (datetime.now() - session_info['start_time']).total_seconds()
    print(f"Total test duration: {total_duration:.2f} seconds")
    
    # Add final performance metrics
    log_performance("test_duration", total_duration, "seconds")
    log_performance("tests_completed", 6, "count")
    log_performance("success_rate", 100.0, "%")
    
    # Close session
    print("\nClosing observability session...")
    client.close_session()
    
    # Final summary
    print("\nüéâ Test Results Summary")
    print("=" * 30)
    
    for test_name, result in test_results.items():
        print(f"‚úÖ {test_name}: PASSED")
    
    print("\nüìà Observability Data Collected:")
    print("- Tool execution traces")
    print("- Performance metrics") 
    print("- Agent decision logs")
    print("- Error handling records")
    print("- Session lifecycle data")
    
    print("\nüîç Check Langfuse and Supabase for detailed observability data")
    print("‚úÖ End-to-end observability test completed successfully!")
    
    return test_results

if __name__ == "__main__":
    main()
